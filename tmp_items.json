{"items":[{"content":{"body":"## Description\n\nThe AI Model Settings page at `/settings/ai-model` is missing the 'AI Engine' header element, causing 3 E2E tests to fail.\n\n## Failed Tests\n\n1. `navigates to AI Model settings page`\n2. `displays provider selector dropdown`\n3. `shows Gemini-specific settings when Gemini is selected`\n\n## Error\n\n```\nError: expect(locator).toBeVisible() failed\nLocator: locator('text=AI Engine')\nExpected: visible\nTimeout: 5000ms\nError: element(s) not found\n```\n\n## Reproduction Steps\n\n1. Navigate to https://icy-desert-07c08ba00.2.azurestaticapps.net/settings/ai-model\n2. Observe that the 'AI Engine' header text is missing\n3. Run E2E tests: `npx playwright test e2e/ai-model.spec.ts`\n4. Tests fail waiting for the element\n\n## Impact\n\n- 3 E2E tests failing\n- Users may be confused about what page they're on\n- Page may be completely broken or showing wrong content\n\n## Environment\n\n- **Staging URL**: https://icy-desert-07c08ba00.2.azurestaticapps.net/\n- **Test run date**: 2026-02-13\n- **Test results**: 3 failed, 7 interrupted, 4 passed\n\n## Investigation Needed\n\n1. Check if the AI Model Settings page exists and renders\n2. Verify the page has proper header/title elements\n3. Check for JavaScript errors on page load\n4. Review recent changes to this page or routing","number":412,"repository":"bradyoo12/ai-dev-request","title":"[UI Bug] AI Model Settings page missing 'AI Engine' header element","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/412"},"id":"PVTI_lAHNf9fOATn4hM4JVjpW","labels":["bug"],"linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/417"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[UI Bug] AI Model Settings page missing 'AI Engine' header element"},{"content":{"body":"## Description\n\nMultiple E2E tests for AI Model Settings are failing on staging because expected UI elements are not visible.\n\n## Failing Tests (8 of 12 AI Model Settings tests fail)\n\n1. **shows Gemini-specific settings when Gemini is selected** - 'text=AI Engine' not visible\n2. **displays provider selector dropdown** - 'text=AI Engine' not visible  \n3. **loads and displays available providers** - select element not visible\n\nAll tests timeout at 5000ms waiting for elements that should be on the page.\n\n## Root Cause Investigation Needed\n\n1. Check if AI Model Settings page exists at /settings/ai-model on staging\n2. Verify the page renders without JavaScript errors\n3. Check if recent changes to #384 (model options) affected the Settings UI\n4. Verify the 'AI Engine' heading is present in the Settings layout\n\n## Expected Behavior\n\n- AI Model Settings page should load successfully\n- 'AI Engine' heading should be visible\n- Provider selector dropdown should be present\n- Tests should pass on staging\n\n## Steps to Reproduce\n\n1. Navigate to https://icy-desert-07c08ba00.2.azurestaticapps.net/settings/ai-model\n2. Observe if page loads correctly\n3. Check browser console for errors\n4. Run: `npm run test:staging -- ai-model.spec.ts`\n\n## Test Output\n\n```\nError: expect(locator).toBeVisible() failed\nLocator: locator('text=AI Engine')\nExpected: visible  \nTimeout: 5000ms\nError: element(s) not found\n```\n\n## Impact\n\n- Blocks verification of #384 (model options feature)\n- 8 E2E tests failing\n- May indicate broken Settings page on staging","number":413,"repository":"bradyoo12/ai-dev-request","title":"[UI Bug] AI Model Settings page not rendering correctly on staging","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/413"},"id":"PVTI_lAHNf9fOATn4hM4JVk_K","labels":["bug"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[UI Bug] AI Model Settings page not rendering correctly on staging"},{"content":{"body":"# Kakao OAuth Login Returns 400 Error\r\n\r\n## Original Request\r\n> cannot continue with Kakao\r\n\r\n## Problem Statement\r\n\r\nUsers attempting to login via Kakao OAuth are experiencing a **400 Bad Request error** when the authorization code is exchanged for an access token. The error manifests as:\r\n\r\n```\r\nFailed to load resource: the server responded with a status of 400 (.)\r\nat WE (index-DF1o7Mz4.js:1s:18)\r\n\r\nSocial login failed: Error: Social login failed. Please try again.\r\nat kE (index-DF1o7Mz4.js:1s:18)\r\n```\r\n\r\n**Impact**: Users cannot authenticate using Kakao, blocking access to the platform for Korean users who primarily use Kakao for social login.\r\n\r\n## Current Implementation\r\n\r\nThe Kakao OAuth flow is implemented in:\r\n- **Backend**: `SocialAuthService.ExchangeKakaoCodeAsync()` at [SocialAuthService.cs:155-202](platform/backend/AiDevRequest.API/Services/SocialAuthService.cs#L155-L202)\r\n- **Frontend**: `LoginPage.handleSocialLogin()` at [LoginPage.tsx:63-74](platform/frontend/src/pages/LoginPage.tsx#L63-L74)\r\n\r\nRecent changes:\r\n- Commit `b1257e8`: Added Kakao `client_secret` to OAuth token exchange (this may have introduced the issue)\r\n\r\n## Root Cause Investigation Needed\r\n\r\nPotential causes:\r\n1. **Configuration Issue**: Kakao OAuth credentials (`OAuth:Kakao:ClientId`, `OAuth:Kakao:ClientSecret`) may be incorrect or missing in production\r\n2. **Redirect URI Mismatch**: The `redirect_uri` sent to Kakao may not match the registered URI in Kakao Developers Console\r\n3. **Client Secret Requirement**: Kakao may not require or may reject `client_secret` for certain app types (Web vs Native)\r\n4. **Scope/Permissions**: Missing required scopes in the authorization URL\r\n5. **Token Endpoint Issue**: Incorrect content-type or parameter format in the token exchange request\r\n\r\n## Success Criteria\r\n\r\n- [ ] Users can successfully complete Kakao OAuth login flow\r\n- [ ] Token exchange with `https://kauth.kakao.com/oauth/token` returns 200 with access token\r\n- [ ] Error handling provides clear feedback if configuration is missing\r\n- [ ] Backend logs include detailed error messages from Kakao API responses for debugging\r\n\r\n## Implementation Guidance\r\n\r\n### Step 1: Verify Kakao App Configuration\r\n1. Check Kakao Developers Console settings:\r\n   - Confirm **Redirect URI** matches exactly: `https://icy-desert-07c08ba00.2.azurestaticapps.net/auth/callback/kakao`\r\n   - Verify **Client Secret** is enabled/disabled correctly for the app type\r\n   - Confirm **Required Scopes** are minimal (Kakao Account, Profile)\r\n\r\n### Step 2: Enhance Error Logging\r\nUpdate `ExchangeKakaoCodeAsync()` to log the full error response:\r\n```csharp\r\nif (!tokenResponse.IsSuccessStatusCode)\r\n{\r\n    _logger.LogError(\"Kakao token exchange failed: Status={Status}, Response={Response}\",\r\n        tokenResponse.StatusCode, tokenJson);\r\n    throw new InvalidOperationException($\"Kakao token exchange failed: {tokenJson}\");\r\n}\r\n```\r\n\r\n### Step 3: Review Token Exchange Request\r\nCompare implementation with [Kakao OAuth Token Docs](https://developers.kakao.com/docs/latest/en/kakaologin/rest-api#request-token):\r\n- Verify `Content-Type: application/x-www-form-urlencoded`\r\n- Confirm `grant_type=authorization_code`\r\n- Check if `client_secret` should be omitted or is required\r\n\r\n### Step 4: Test with Detailed Logs\r\n1. Deploy changes to staging\r\n2. Attempt Kakao login and capture full backend logs\r\n3. Inspect the actual error message from Kakao API\r\n4. Adjust configuration based on specific error\r\n\r\n## Out of Scope\r\n\r\n- Implementing fallback authentication methods\r\n- Refactoring the entire OAuth flow\r\n- Supporting Kakao Talk Business or Enterprise accounts (unless needed)\r\n\r\n## Dependencies\r\n\r\n- Access to Kakao Developers Console to verify app configuration\r\n- Access to Azure Container Apps logs to inspect production errors\r\n- May block Korean user onboarding until resolved\r\n\r\n## Technical Context\r\n\r\n**Kakao OAuth Endpoints**:\r\n- Authorization: `https://kauth.kakao.com/oauth/authorize`\r\n- Token Exchange: `https://kauth.kakao.com/oauth/token`\r\n- User Info: `https://kapi.kakao.com/v2/user/me`\r\n\r\n**Configuration Keys**:\r\n```json\r\n{\r\n  \"OAuth\": {\r\n    \"Kakao\": {\r\n      \"ClientId\": \"[REST API Key from Kakao Console]\",\r\n      \"ClientSecret\": \"[Optional: Only if enabled in Console]\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Related Issues\r\n\r\n- Commit `b1257e8`: Recent fix that added `client_secret` - verify this change is correct\r\n- May need to verify Google/LINE/Apple OAuth flows still work after any changes to shared auth code\r\n","number":403,"repository":"bradyoo12/ai-dev-request","title":"Kakao OAuth Login Returns 400 Error","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/403"},"id":"PVTI_lAHNf9fOATn4hM4JVht-","linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/411"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"In review","title":"Kakao OAuth Login Returns 400 Error"},{"content":{"body":"","number":406,"repository":"bradyoo12/ai-dev-request","title":"Fix OAuth button layout shift on login page","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/406"},"id":"PVTI_lAHNf9fOATn4hM4JVh41","linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/409"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"Fix OAuth button layout shift on login page"},{"content":{"body":"## Original Request\r\n\r\nadd playwright tests and get them run whenever new implementation is done by updating b-start.md or b-ready\r\n\r\n## Problem Statement\r\n\r\nThe current automated development workflows (`b-ready` and `b-review`) **run** Playwright E2E tests, but they don't automatically **create or update** Playwright tests when new features are implemented. The `unit-test-analyst` agent (introduced in `b-start.md` lines 248-267) only creates unit tests, not E2E tests.\r\n\r\n**Current state:**\r\n- ✅ Playwright tests run locally in `b-ready` (Step 6b)\r\n- ✅ Playwright tests run against staging in `b-review` (Step 3a)\r\n- ❌ No automated creation of Playwright tests for new features\r\n- ❌ Existing Playwright tests may become outdated as features evolve\r\n\r\n**Existing Playwright tests:**\r\n- `platform/frontend/e2e/accessibility.spec.ts`\r\n- `platform/frontend/e2e/form.spec.ts`\r\n- `platform/frontend/e2e/homepage.spec.ts`\r\n- `platform/frontend/e2e/i18n.spec.ts`\r\n- `platform/frontend/e2e/navigation.spec.ts`\r\n\r\n## Success Criteria\r\n\r\n1. **E2E Test Analyst Agent**: Create an `e2e-test-analyst` agent that:\r\n   - Identifies new user-facing features (pages, routes, forms, workflows) added in the current ticket\r\n   - Checks if corresponding Playwright E2E tests exist\r\n   - Creates or updates Playwright tests to cover new functionality\r\n   - Follows existing test patterns in `platform/frontend/e2e/`\r\n   - Runs the new tests to verify they pass\r\n\r\n2. **Workflow Integration**: Update `b-start.md` and `b-ready.md` to include the `e2e-test-analyst` agent:\r\n   - Insert after `unit-test-analyst` completes (before the `tester` agent runs)\r\n   - The `tester` agent will then run ALL tests (including newly created E2E tests)\r\n\r\n3. **Test Coverage Guidelines**: Define what should be covered by Playwright E2E tests:\r\n   - New pages and routes\r\n   - Critical user workflows (login, form submission, navigation)\r\n   - Integration between components (API calls + UI updates)\r\n   - Accessibility features (keyboard navigation, ARIA labels)\r\n   - i18n/localization for new UI strings\r\n\r\n4. **Test Execution**: Ensure newly created tests are automatically run:\r\n   - Locally in `b-ready` (Step 6b) before PR creation\r\n   - Against staging in `b-review` (Step 3a) before moving to Done\r\n\r\n5. **Automatic Bug Ticket Creation**: When tests fail after fix attempts:\r\n   - Create a GitHub issue for each failing test with `bug` label\r\n   - Include test name, failure details, reproduction steps, and error logs\r\n   - Add ticket to Project 26 with \"Ready\" status\r\n   - Link the bug ticket to the original feature ticket\r\n\r\n## Implementation Guidance\r\n\r\n### Changes Required\r\n\r\n#### 1. Update `b-start.md` (Step 3c)\r\nAdd the `e2e-test-analyst` agent between `unit-test-analyst` and `tester`:\r\n\r\n```markdown\r\n6. After frontend-dev and backend-dev complete, spawn **unit-test-analyst** agent...\r\n   [existing content]\r\n\r\n7. After unit-test-analyst completes, spawn **e2e-test-analyst** agent (general-purpose, team_name: ready-<ticket_number>):\r\n   - Identifies new user-facing features added by the current ticket\r\n   - For each new feature, checks if Playwright E2E tests exist in platform/frontend/e2e/\r\n   - Analyzes what should be covered by E2E tests:\r\n     - New pages and routes\r\n     - Form submissions and validations\r\n     - Navigation flows\r\n     - API integration (user action → network request → UI update)\r\n     - Critical user workflows (multi-step processes)\r\n     - Accessibility features (keyboard nav, screen reader labels)\r\n   - Creates or updates Playwright tests following existing patterns\r\n   - Runs E2E tests locally to verify they pass: `npm test` in platform/frontend\r\n   - If new tests fail, fixes them (up to 3 attempts)\r\n   - **If tests still fail after 3 attempts, create a bug ticket:**\r\n     ```bash\r\n     gh api --method POST \"repos/bradyoo12/ai-dev-request/issues\" \\\r\n       -f title=\"[E2E Test Failure] {test name}\" \\\r\n       -f body=\"## Failing Test\\n\\n{test file and name}\\n\\n## Error\\n\\n{error details}\\n\\n## Steps to Reproduce\\n\\n{steps}\\n\\n## Related\\n\\nRefs #{original_ticket_number}\" \\\r\n       -f \"labels[]=bug\"\r\n     # Add to project with Ready status (use hardcoded IDs from policy.md)\r\n     ```\r\n   - Reports results to planner: how many tests added/updated\r\n\r\n8. After e2e-test-analyst completes, spawn **tester** agent...\r\n   [existing content — tester now runs ALL tests including new E2E tests]\r\n```\r\n\r\n#### 2. Update `b-ready.md` (Step 5)\r\nAdd guidance for E2E test creation in standalone mode:\r\n\r\n```markdown\r\n### Step 5: Implement the Plan\r\n1. Make all necessary code changes\r\n2. Follow existing project patterns\r\n3. Write clean, well-documented code\r\n4. **Create or update Playwright E2E tests** for new user-facing features:\r\n   - New pages/routes → test navigation and rendering\r\n   - New forms → test submission and validation\r\n   - New workflows → test end-to-end user journey\r\n   - Place tests in platform/frontend/e2e/ following existing patterns\r\n\r\n**In team mode:** If you are a specialized agent (frontend-dev or backend-dev), only implement your assigned scope. Use SendMessage to report completion to the planner/team lead.\r\n```\r\n\r\n#### 3. Update `b-review.md` (Step 3a)\r\nAdd automatic bug ticket creation for staging test failures:\r\n\r\n```markdown\r\n#### Step 3a: Run FULL Playwright E2E Test Suite Against Staging\r\n```bash\r\ncd platform/frontend\r\nnpx playwright install chromium\r\nnpm run test:staging\r\n```\r\n\r\n**If tests fail:**\r\n1. Analyze the failure to determine if it's a regression or environment issue\r\n2. For each failing test (up to 5 max per cycle):\r\n   - Create a bug ticket with detailed failure information:\r\n     ```bash\r\n     gh api --method POST \"repos/bradyoo12/ai-dev-request/issues\" \\\r\n       -f title=\"[Staging Test Failure] {test name}\" \\\r\n       -f body=\"## Test\\n\\n{test file}::{test name}\\n\\n## Environment\\n\\nStaging: https://icy-desert-07c08ba00.2.azurestaticapps.net\\n\\n## Error\\n\\n{failure message and stack trace}\\n\\n## Screenshots/Trace\\n\\n{Playwright trace/screenshot URLs if available}\\n\\n## Related Ticket\\n\\nFound during verification of #{ticket_number}\" \\\r\n       -f \"labels[]=bug\"\r\n     ```\r\n   - Add ticket to Project 26 with Ready status\r\n3. Report all failures to team lead (in team mode) or add comment to ticket (standalone)\r\n```\r\n\r\n#### 4. Document Test Patterns\r\nAdd a section to `.claude/design.md` or create `.claude/testing-guide.md` with:\r\n- When to write unit tests vs E2E tests\r\n- Playwright test file naming conventions\r\n- Example E2E test structure\r\n- How to run tests locally and against staging\r\n- How bug tickets are auto-created for test failures\r\n\r\n### Example E2E Test Scenarios\r\n\r\n**New page added** → Create test:\r\n```typescript\r\n// e2e/new-feature.spec.ts\r\ntest('should navigate to new feature page', async ({ page }) => {\r\n  await page.goto('/');\r\n  await page.click('text=New Feature');\r\n  await expect(page).toHaveURL('/new-feature');\r\n  await expect(page.locator('h1')).toContainText('New Feature');\r\n});\r\n```\r\n\r\n**Form submission** → Create test:\r\n```typescript\r\ntest('should submit form and show success message', async ({ page }) => {\r\n  await page.goto('/form');\r\n  await page.fill('input[name=\"name\"]', 'Test User');\r\n  await page.click('button[type=\"submit\"]');\r\n  await expect(page.locator('.success-message')).toBeVisible();\r\n});\r\n```\r\n\r\n## Out of Scope\r\n\r\n- Retroactively creating E2E tests for existing features (do this in a separate ticket)\r\n- Playwright test configuration changes (already working)\r\n- Visual regression testing (may be added later)\r\n- Performance testing (separate concern)\r\n\r\n## Dependencies\r\n\r\n- Existing Playwright setup in `platform/frontend/playwright.config.ts`\r\n- `b-start.md` orchestrator workflow\r\n- `b-ready.md` implementation workflow\r\n- `unit-test-analyst` agent pattern (use as reference)\r\n\r\n## Related Issues\r\n\r\n- None currently — this is a workflow improvement\r\n\r\n## Notes\r\n\r\n- The `e2e-test-analyst` agent should reuse patterns from `unit-test-analyst` (lines 248-267 in `b-start.md`)\r\n- E2E tests are more expensive than unit tests — focus on critical user paths, not every component\r\n- Tests run twice: locally in `b-ready` (Step 6b) and against staging in `b-review` (Step 3a)\r\n- **Auto-created bug tickets** prevent test failures from blocking the pipeline while ensuring issues are tracked\r\n- Bug tickets for test failures should include:\r\n  - Test file and test name\r\n  - Full error message and stack trace\r\n  - Steps to reproduce\r\n  - Link to original feature ticket\r\n  - Playwright trace/screenshot URLs when available\r\n- Limit bug ticket creation to 5 per cycle to avoid overwhelming the backlog\r\n- If E2E test creation fails repeatedly, add `on hold` label and let human review the requirement","number":408,"repository":"bradyoo12/ai-dev-request","title":"Add Playwright E2E test creation to automated workflows","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/408"},"id":"PVTI_lAHNf9fOATn4hM4JViVl","linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/410"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"Add Playwright E2E test creation to automated workflows"},{"content":{"body":"## Description\n\nTwo E2E tests consistently fail on staging due to the page never reaching networkidle state:\n\n1. `Homepage › has no unexpected console errors` \n2. `Internationalization › app renders without showing raw i18n keys`\n\nBoth fail with:\n```\nTest timeout of 30000ms exceeded\nError: page.waitForLoadState: Test timeout of 30000ms exceeded\n```\n\n## Impact\n\n- 2 of 21 E2E tests fail (9.5% failure rate)\n- Indicates ongoing network requests that never complete\n- May be related to the translation API issue from #380, or a new regression\n\n## Environment\n\n- **Staging URL**: https://icy-desert-07c08ba00.2.azurestaticapps.net/\n- **Test run date**: 2026-02-13\n- **Test results**: 17 passed, 2 failed, 2 skipped\n\n## Investigation Needed\n\n1. Check browser console for ongoing network requests\n2. Verify translation API endpoints are working\n3. Check if there are any long-polling or SSE connections that don't close\n4. Review any recent changes that might affect page loading","number":404,"repository":"bradyoo12/ai-dev-request","title":"[Bug] Staging site fails to reach networkidle state, causing E2E test timeouts","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/404"},"id":"PVTI_lAHNf9fOATn4hM4JVhxy","labels":["bug"],"linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/407"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Bug] Staging site fails to reach networkidle state, causing E2E test timeouts"},{"content":{"body":"Starting with Gemini for now, I want to give users options to select the AI model. Add more options like Sonnet of Cluade, and so on to each user. ","number":384,"repository":"bradyoo12/ai-dev-request","title":"model options","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/384"},"id":"PVTI_lAHNf9fOATn4hM4JU121","repository":"https://github.com/bradyoo12/ai-dev-request","status":"In progress","title":"model options"},{"content":{"body":"Currently the price still says '149,000WON' when the language English is selected.\n\nWhen the user is logged in and preferred 통화 is set, use that 통화.\nWhen the user is not logged in or the user is logged in and prefered 통화 is not set, lotate where the user is and set the 통화 of the country where the user is.\n\n<img width=\"1357\" height=\"872\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fc7def98-b291-41df-a89a-2056eb27790a\" />","number":381,"repository":"bradyoo12/ai-dev-request","title":"change to USD when language selected is USA","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/381"},"id":"PVTI_lAHNf9fOATn4hM4JU1dz","linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/402"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"change to USD when language selected is USA"},{"content":{"body":"currently https://icy-desert-07c08ba00.2.azurestaticapps.net/buy-credits is not working. fix it\n\n<img width=\"1429\" height=\"849\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c3968822-fefd-4004-a819-2cc35ea51f4a\" />\n\n<img width=\"1930\" height=\"1288\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8b7669fb-1f3f-45c2-bace-25fdaae5fadc\" />","number":382,"repository":"bradyoo12/ai-dev-request","title":"investigate why not working","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/382"},"id":"PVTI_lAHNf9fOATn4hM4JU1tJ","linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/416"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"In progress","title":"investigate why not working"},{"content":{"body":"## Overview\n\nAdd git-like database branching for generated projects, enabling instant isolated database clones per version/PR. Each preview deployment gets its own database branch with production-like data.\n\n## Motivation\n\nCurrently, generated projects share a single database state. When users iterate on their project (rebuild, modify schema), there's no way to preview changes against an isolated database. Database branching enables safe experimentation without affecting the main data.\n\n## Key Features\n\n- **Instant database cloning**: Copy-on-write technology provisions DB branches in seconds regardless of size\n- **Per-version isolation**: Each project version/rebuild gets its own database branch\n- **Merge/discard**: Users can merge schema changes from a branch back to main or discard\n- **Schema migration tracking**: Auto-detect and apply schema diffs between branches\n\n## Implementation Options\n\n- **Neon PostgreSQL**: Native branching support, scale-to-zero, 80%+ of Neon databases are created by AI agents\n- **Supabase Branching**: Built-in branching with RLS, vector search, Edge Functions\n- **DBLab Engine**: Self-hosted PostgreSQL branching with thin cloning\n\n## Competitor Reference\n\n- **Supabase**: Database branching integrated with their platform\n- **Neon**: Instant database branching used by AI development tools\n- **Lovable.dev**: Preview environments with data isolation\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 4/5 |\n| Impact | 4/5 |\n| Effort | 4/5 |","number":387,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Database branching with preview environments per generated project version","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/387"},"id":"PVTI_lAHNf9fOATn4hM4JU5VA","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Database branching with preview environments per generated project version"},{"content":{"body":"## Overview\n\nImplement self-healing test automation that automatically detects and fixes broken tests when the generated project's UI or code changes, reducing test maintenance overhead by up to 95%.\n\n## Motivation\n\nWhen AI generates or modifies code in a project, existing tests often break due to changed selectors, updated component structure, or modified API responses. Currently, broken tests require manual intervention. Self-healing tests would automatically adapt to changes.\n\n## Key Features\n\n- **Intent-based test locators**: Instead of brittle CSS selectors, use AI to understand the semantic intent of test assertions\n- **Auto-fix on UI change**: When a component's structure changes, AI analyzes the diff and updates test selectors/assertions automatically\n- **Confidence scoring**: Each auto-healed test gets a confidence score; low-confidence fixes are flagged for human review\n- **Integration with existing Playwright E2E**: Works alongside the current Playwright test infrastructure\n\n## Competitor Reference\n\n- **Replit Agent 3**: Self-healing loop that tests built apps in a live browser and fixes issues autonomously\n- **mabl**: Adaptive auto-healing that eliminates up to 95% of test maintenance\n- **Virtuoso QA**: Self-healing AI automatically updates test scripts when UI elements change\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 4/5 |\n| Impact | 4/5 |\n| Effort | 3/5 |","number":386,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Self-healing test automation for generated projects","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/386"},"id":"PVTI_lAHNf9fOATn4hM4JU5Uc","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Self-healing test automation for generated projects"},{"content":{"body":"## Description\r\n\r\nThe staging site at https://icy-desert-07c08ba00.2.azurestaticapps.net/ is permanently stuck on a \"Loading...\" screen. No app content ever renders.\r\n\r\n## Root Cause\r\n\r\nThe i18n configuration uses i18next-http-backend to fetch translations from the backend API:\r\n- GET /api/translations/en returns **500 Internal Server Error**\r\n- GET /api/translations/ko returns an **empty object** {}\r\n\r\nAlthough translations are bundled client-side in src/locales/en.json and ko.json, the HTTP backend plugin blocks React Suspense resolution, causing the permanent loading state.\r\n\r\nThe backend health check (/health) returns \"Healthy\", so the issue is specific to the translation endpoints.\r\n\r\n## Reproduction Steps\r\n1. Navigate to https://icy-desert-07c08ba00.2.azurestaticapps.net/\r\n2. Observe the page shows \"Loading...\" indefinitely\r\n\r\n## Fix Options\r\n1. Fix backend /api/translations/lng endpoint to return proper translation data\r\n2. Make HTTP backend non-blocking: Configure i18next so bundled resources take priority and HTTP backend failures don't block app initialization\r\n3. Both: Fix the backend AND add resilient fallback\r\n\r\n## E2E Test Impact\r\n- 2 tests fail (networkidle timeout)\r\n- 10 tests skip (header never renders)\r\n- Only 9 of 21 tests pass","number":380,"repository":"bradyoo12/ai-dev-request","title":"[P0 Bug] Staging site stuck on Loading screen due to broken translation API","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/380"},"id":"PVTI_lAHNf9fOATn4hM4JU0L1","labels":["bug"],"linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/385"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[P0 Bug] Staging site stuck on Loading screen due to broken translation API"},{"content":{"body":"## Overview\n\nOpenTelemetry has introduced semantic extensions specifically for AI agent observability. These standardized extensions enable monitoring of agent execution, tool calls, token usage, and inter-agent communication — critical for understanding why AI agents make certain decisions and where failures occur.\n\n## Why This Matters for AI Dev Request\n\n- **Debugging**: Trace entire AI pipeline from request analysis through code generation to identify failures\n- **Cost Visibility**: Track token usage per request stage (analysis, proposal, generation) for cost optimization\n- **Performance**: Measure latency at each pipeline stage, identify bottlenecks\n- **Production Reliability**: Correlate AI agent telemetry with application-level metrics\n\n## Implementation\n\n1. Add OpenTelemetry .NET SDK to backend\n2. Instrument AI service calls with custom spans (analysis, proposal, generation)\n3. Add token usage metrics as span attributes\n4. Export to Application Insights (already on Azure)\n5. Add a basic observability dashboard in admin panel\n\n## Scores\n\n- **Relevance**: 4/5 — essential for production AI systems\n- **Impact**: 3/5 — improves debugging and cost tracking\n- **Effort**: 2/5 — .NET has excellent OpenTelemetry support\n\n## Sources\n\n- [OpenTelemetry Extensions for AI Agents](https://devops.com/opentelemetry-extensions-to-enable-observability-of-ai-agents/)\n- [.NET OpenTelemetry Documentation](https://learn.microsoft.com/en-us/dotnet/core/diagnostics/observability-with-otel)","number":363,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Add OpenTelemetry instrumentation for AI agent observability","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/363"},"id":"PVTI_lAHNf9fOATn4hM4JUyPc","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Add OpenTelemetry instrumentation for AI agent observability"},{"content":{"body":"## Overview\n\nCompetitors like Cursor (BugBot) and Qodo 2.0 now offer multi-agent automated PR code review. By February 2026, 41% of all code is AI-generated but only 25% of organizations have enterprise-wide testing — creating a code review bottleneck that automated review agents solve.\n\n## Why This Matters for AI Dev Request\n\n- **Quality Gate**: Automatically review AI-generated code before delivery to users\n- **Trust Building**: Users see that generated code has been independently reviewed\n- **Bug Prevention**: Catch security vulnerabilities, performance issues, and design flaws before deployment\n- **Competitive Parity**: Cursor BugBot and Qodo 2.0 have set this as table-stakes\n\n## Implementation\n\n1. After code generation, trigger an AI review pass using Claude API\n2. Review across 5 dimensions: security, performance, accessibility, architecture, maintainability\n3. Generate a review summary with findings and severity ratings\n4. Auto-fix critical issues before delivering to users\n5. Show review results in the project dashboard\n\n## Scores\n\n- **Differentiation**: 4/5 — builds user trust in AI-generated code\n- **User Value**: 5/5 — directly improves code quality\n- **Feasibility**: 3/5 — leverages existing Claude API integration\n\n## Sources\n\n- [Cursor BugBot](https://prismic.io/blog/cursor-ai)\n- [Qodo 2.0 AI Code Review](https://www.qodo.ai/blog/best-ai-code-review-tools-2026/)","number":364,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Automated AI code review agent for generated project pull requests","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/364"},"id":"PVTI_lAHNf9fOATn4hM4JUyP-","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Automated AI code review agent for generated project pull requests"},{"content":{"body":"Duplicate of #380. Closing to consolidate.","number":379,"repository":"bradyoo12/ai-dev-request","title":"[P0 Bug] Staging site stuck on Loading... screen due to broken translation API","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/379"},"id":"PVTI_lAHNf9fOATn4hM4JU0LQ","labels":["bug"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[P0 Bug] Staging site stuck on Loading... screen due to broken translation API"},{"content":{"body":"## Overview\n\nAdd sandboxed code execution environments (MicroVM/gVisor) for safely running and testing AI-generated code before deployment, treating all generated code as untrusted by default.\n\n## Motivation\n\nWhen the platform generates code for users, that code currently cannot be safely executed or tested in an isolated environment. Running AI-generated code without sandboxing creates security risks: potential secret exposure, resource exhaustion, container escape, or malicious operations from bugs/hallucinations.\n\n## Key Features\n\n- **Isolated execution environment**: Each generated project gets a sandboxed runtime (MicroVM or container with gVisor)\n- **Network egress controls**: Block arbitrary network access to prevent data exfiltration\n- **Filesystem isolation**: Write operations restricted to project workspace only\n- **Resource limits**: CPU, memory, and time limits per execution\n- **Live browser testing**: Run generated web apps in headless browser within sandbox (like Replit Agent 3's self-healing loop)\n- **Build verification**: Compile and run tests in sandbox before presenting results to user\n\n## Implementation Options\n\n- **Firecracker MicroVMs**: Strongest isolation with dedicated kernels (used by AWS Lambda)\n- **gVisor**: User-space kernel, lighter than MicroVMs, syscall interception\n- **Deno Sandbox**: Sub-1-second startup, built-in permission system for JS/TS projects\n\n## Competitor Reference\n\n- **Replit Agent 3**: 200-minute autonomous runtime with self-healing loop testing in live browser\n- **Bolt.new**: WebContainer-based sandboxed execution in browser\n- **Cursor**: Terminal access with sandboxed execution\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 5/5 |\n| Impact | 5/5 |\n| Effort | 4/5 |","number":391,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Sandboxed code execution for AI-generated project testing","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/391"},"id":"PVTI_lAHNf9fOATn4hM4JU6o1","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Sandboxed code execution for AI-generated project testing"},{"content":{"body":"## Overview\n\nAdd Replit-style Dynamic Intelligence to the AI analysis and generation pipeline, allowing users to select reasoning depth and model power level for their dev requests.\n\n## Key Features\n\n- **Extended Thinking Mode**: Show step-by-step AI reasoning before delivering the final analysis/proposal, helping users understand the AI's decision-making process\n- **Power Level Selection**: Let users choose between Standard (fast, cheaper) and High Power (most capable model, 5x cost) modes depending on project complexity\n- **Web Search Integration**: Automatically search the web for latest documentation, libraries, and best practices during analysis\n- **Transparent Cost**: Show estimated token cost before starting, with per-mode multipliers\n\n## Motivation\n\nReplit Agent 3's Dynamic Intelligence demonstrates that users want control over AI reasoning depth. Complex enterprise projects benefit from extended thinking, while simple sites should be fast and cheap. This aligns costs with value delivered.\n\n## Implementation\n\n- Frontend: Add power level selector (Standard/Extended/High Power) to the dev request form\n- Backend: Route to different Claude models/configurations based on selection\n- AI Engine: Implement extended thinking prompts with streaming reasoning output\n- Billing: Apply cost multipliers per mode (1x/2x/5x)\n\n## Competitor Reference\n\n- **Replit Agent 3**: Extended Thinking + High Power + Web Search (Dynamic Intelligence)\n- **Cursor**: Tab/Copilot++ for different reasoning levels\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 5/5 |\n| Impact | 5/5 |\n| Effort | 3/5 |","number":392,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Dynamic Intelligence with extended thinking and model power selection","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/392"},"id":"PVTI_lAHNf9fOATn4hM4JU6sC","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Dynamic Intelligence with extended thinking and model power selection"},{"content":{"body":"## Overview\n\nImplement effort-based and outcome-based pricing as an alternative to fixed subscription plans, charging users based on actual AI compute consumed and the complexity of generated projects.\n\n## Key Features\n\n- **Effort-Based Billing**: Charge based on AI agent runtime minutes and model usage, similar to Replit's approach (which grew revenue from $10M to $100M in 9 months)\n- **Outcome Tiers**: Different pricing for successful builds vs. failed attempts, aligning cost with delivered value\n- **Credit Packs**: Pre-purchased credit bundles for predictable budgeting alongside usage metering\n- **Real-Time Usage Dashboard**: Show live token consumption, cost breakdown by project, and spending alerts\n\n## Motivation\n\nUsage-based pricing has become standard for AI platforms (OpenAI, Anthropic). Replit's switch to effort-based pricing was transformative for their business. Fair pricing that scales with project complexity attracts both hobbyists (small projects = low cost) and enterprises (complex builds = premium value).\n\n## Implementation\n\n- Backend: Implement metering infrastructure for tracking AI compute per request\n- Billing: Integrate with Stripe Billing or Metronome for usage-based invoicing\n- Frontend: Add usage dashboard with cost breakdown and spending controls\n- Pricing page: Show hybrid plans (base subscription + metered AI usage)\n\n## Competitor Reference\n\n- **Replit**: Effort-based pricing with agent runtime billing\n- **Lovable**: Token-based pricing with per-message costs\n- **OpenAI/Anthropic**: Pure usage-based API pricing\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 4/5 |\n| Impact | 5/5 |\n| Effort | 4/5 |","number":393,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Effort-based pricing with usage metering and outcome billing","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/393"},"id":"PVTI_lAHNf9fOATn4hM4JU6sh","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Effort-based pricing with usage metering and outcome billing"},{"content":{"body":"## Overview\n\nImplement Factory.ai-style automated agent triggering from GitHub issue assignment, where AI agents automatically pick up assigned issues, pull context, implement solutions, and create PRs with full traceability.\n\n## Key Features\n\n- **GitHub Webhook Integration**: Listen for issue assignment events and auto-trigger AI analysis\n- **Automatic Context Gathering**: Pull issue description, linked PRs, relevant code files, and project history\n- **Autonomous Implementation**: AI agent creates a branch, implements changes, runs tests, and opens a PR\n- **Full Traceability**: Link every code change back to the originating issue with audit trail\n- **Parallelized Agent Tasks**: Run multiple agents on different issues simultaneously for backlog clearing\n\n## Motivation\n\nFactory.ai has demonstrated that agent-triggered automation from issue tracking is a key differentiator for enterprise customers. Automating the issue-to-PR pipeline eliminates manual handoffs and dramatically speeds up maintenance and feature work.\n\n## Implementation\n\n- Backend: Add GitHub webhook handler for issue events (assigned, labeled)\n- AI Engine: Create issue-to-implementation pipeline using Claude API\n- Integration: Use existing GitHub sync infrastructure to push branches and create PRs\n- Frontend: Add dashboard showing auto-triggered agent activity and status\n\n## Competitor Reference\n\n- **Factory.ai**: Agents auto-trigger from GitHub issue assignment, create PRs with full traceability\n- **Devin**: Autonomous software engineer that implements from specs\n- **Replit Agent 3**: 200-minute autonomous coding sessions\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 5/5 |\n| Impact | 5/5 |\n| Effort | 4/5 |","number":394,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Agent-triggered automation from GitHub issue assignment","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/394"},"id":"PVTI_lAHNf9fOATn4hM4JU6s8","labels":["suggestion"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Done","title":"[Suggestion] Agent-triggered automation from GitHub issue assignment"},{"content":{"body":"## Overview\n\nImplement v0-style branch-per-chat git workflow where each dev request conversation automatically maps to a dedicated git branch with real-time two-way sync between the platform and local development.\n\n## Key Features\n\n- **Auto Branch Creation**: Each new dev request chat automatically creates a git branch, with every AI-generated code change auto-committed\n- **Two-Way Sync**: Changes made locally in an IDE are pulled into the platform, and platform changes push to the branch — enabling seamless collaboration between AI and human developers\n- **PR-First Workflow**: Generated code goes through pull requests to protect the main branch, with automatic preview deployments per branch\n- **Non-Engineer Collaboration**: Product managers and designers can iterate on projects through the chat interface while engineers review via standard git workflows\n\n## Motivation\n\nVercel's v0 demonstrated that mapping AI chat sessions to git branches dramatically improves the development workflow. Users get version control \"for free\" without managing branches manually, and teams can collaborate through familiar PR-based reviews. This aligns costs with professional development practices.\n\n## Implementation\n\n- Frontend: Add git branch indicator to dev request chat, show commit history timeline\n- Backend: Auto-create branches on new requests, commit on each generation step\n- Integration: Two-way sync using existing GitHub sync infrastructure\n- Preview: Deploy branch previews using existing Azure Static Web Apps\n\n## Competitor Reference\n\n- **v0 (Vercel)**: Branch-per-chat with two-way git sync, auto-commits, PR workflow\n- **Replit**: Git integration with automatic commits per agent action\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 5/5 |\n| Impact | 5/5 |\n| Effort | 3/5 |","number":399,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Branch-per-chat git workflow with two-way sync for dev requests","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/399"},"id":"PVTI_lAHNf9fOATn4hM4JVgap","labels":["suggestion","on hold"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Ready","title":"[Suggestion] Branch-per-chat git workflow with two-way sync for dev requests"},{"content":{"body":"## Overview\n\nAdd Cursor-style parallel subagent orchestration to the AI code generation pipeline, splitting complex builds into specialized parallel agents (frontend, backend, tests, docs) for dramatically faster project generation.\n\n## Key Features\n\n- **Parallel Task Decomposition**: Split a dev request into independent subtasks (UI components, API endpoints, database schema, tests) that run simultaneously\n- **Specialized Subagents**: Each subagent uses focused context and model configuration optimized for its task type (e.g., frontend agent uses UI-focused prompts)\n- **Progress Dashboard**: Real-time visualization showing all active subagents, their progress, and how they merge results\n- **Conflict Resolution**: Automatic merge of parallel outputs with intelligent conflict detection and resolution\n\n## Motivation\n\nCursor 2.0's subagent system showed 40% faster task completion in multi-task scenarios. For AI Dev Request, a typical full-stack project generation could be split into 3-4 parallel streams, reducing generation time from minutes to seconds for complex projects.\n\n## Implementation\n\n- AI Engine: Implement task decomposition logic that identifies parallelizable work\n- Backend: Add subagent orchestration with task queue and result aggregation\n- Frontend: Real-time progress dashboard showing parallel agent activity\n- Infrastructure: Manage concurrent Claude API calls with rate limiting\n\n## Competitor Reference\n\n- **Cursor 2.0**: Subagents run in parallel with independent context, 40% faster execution\n- **Devin + Windsurf**: Multiple agents on different issues simultaneously\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 5/5 |\n| Impact | 4/5 |\n| Effort | 3/5 |","number":400,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] Parallel subagent orchestration for faster code generation","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/400"},"id":"PVTI_lAHNf9fOATn4hM4JVgbN","labels":["suggestion"],"linked pull requests":["https://github.com/bradyoo12/ai-dev-request/pull/415"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"In review","title":"[Suggestion] Parallel subagent orchestration for faster code generation"},{"content":{"body":"## Overview\n\nUpgrade the existing AI code review from single-pass diff analysis to a system-aware multi-agent review pipeline that understands cross-file dependencies, enforces architectural standards, and provides risk scores for every change.\n\n## Key Features\n\n- **Multi-Agent Review Pipeline**: Specialized review agents for security, performance, architecture, and test coverage run in parallel on each PR\n- **System-Aware Analysis**: Reviewers understand the full project architecture (from design.md), not just the diff — catching issues like broken contracts, missing error handling across boundaries, and architectural drift\n- **Risk Scoring**: Each PR receives a composite risk score (0-100) based on complexity, files changed, test coverage delta, and security surface area\n- **Auto-Generated Test Suggestions**: When review identifies untested code paths, automatically suggest or generate missing tests\n\n## Motivation\n\nQodo's multi-agent code review platform demonstrated that specialized agents achieve 42-48% better bug detection than single-pass review. The code review automation market grew from $550M to $4B, showing massive demand. For AI Dev Request, system-aware review ensures generated projects meet quality standards before delivery.\n\n## Implementation\n\n- AI Engine: Create specialized review agent prompts (security, performance, architecture, testing)\n- Backend: Parallel review orchestration with result aggregation and risk score computation\n- Frontend: Review dashboard showing per-agent findings, risk score badge, and suggested fixes\n- Integration: Hook into existing code review pipeline and PR workflow\n\n## Competitor Reference\n\n- **Qodo 2.0**: Multi-agent code review with specialized agents matching senior engineer quality\n- **Cursor Bugbot**: Auto-analyzes PRs for logic bugs, edge cases, and security issues\n\n## Scoring\n\n| Criteria | Score |\n|----------|-------|\n| Relevance | 4/5 |\n| Impact | 5/5 |\n| Effort | 3/5 |","number":401,"repository":"bradyoo12/ai-dev-request","title":"[Suggestion] System-aware multi-agent code review with risk scoring","type":"Issue","url":"https://github.com/bradyoo12/ai-dev-request/issues/401"},"id":"PVTI_lAHNf9fOATn4hM4JVgcP","labels":["suggestion","on hold"],"repository":"https://github.com/bradyoo12/ai-dev-request","status":"Ready","title":"[Suggestion] System-aware multi-agent code review with risk scoring"}],"totalCount":22}
